<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ray's Thoughts and Writings</title><link href="http://www.raydevblog.us/" rel="alternate"></link><link href="http://www.raydevblog.us/feeds/system-design.atom.xml" rel="self"></link><id>http://www.raydevblog.us/</id><updated>2014-07-20T22:07:00-07:00</updated><entry><title>Dynamo: Amazon's Highly Available Key-Value Store</title><link href="http://www.raydevblog.us/posts/2014/dynamo-amazons-highly-available-key-value-store.html" rel="alternate"></link><updated>2014-07-20T22:07:00-07:00</updated><author><name>Ray Chen</name></author><id>tag:www.raydevblog.us,2014-07-20:posts/2014/dynamo-amazons-highly-available-key-value-store.html</id><summary type="html">&lt;p&gt;This &lt;a href="http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html"&gt;paper&lt;/a&gt; was first released in SOSP'07, describes Dynamo, the underlying storage technology for several core services in Amazon's e-commerce platform. Since then, Several Dynamo-inspired databases have appeared (either entirely or partially) by this paper, such as Riak, Cassandra and Voldemort. Hence, I decide to read this paper and briefly describe some well-know technologies implemented by Dynamo.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Dynamo is a completely decentralized system targeting applications that operate with weaker consistency and high availability. It is built to be an &lt;strong&gt;"always writeable"&lt;/strong&gt; data store. Hence, Dynamos allows conflicting updates in the system. &lt;/p&gt;
&lt;p&gt;Dynamo achieves scalability and availability by the following technologies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data is partitioned and replicated using consistent hashing.&lt;/li&gt;
&lt;li&gt;Consistency is facilitated by object versioning. &lt;/li&gt;
&lt;li&gt;The consistency among replicas during updates is maintained by a quorum-like technique and a decentralized replica synchronization protocol. &lt;/li&gt;
&lt;li&gt;A gossip based distributed failure detection and membership protocol. &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;System Architecture&lt;/h3&gt;
&lt;p&gt;Table 1 presents a summary of the list of techniques Dynamo uses and their respective advantages.
&lt;img alt="Alt text" src="http://www.raydevblog.us/images/dynamo.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;The paper gives details on the partitioning, replication, versioning, membership, and failure handling components of Dynamo.&lt;/p&gt;
&lt;h3&gt;Partitioning&lt;/h3&gt;
&lt;h4&gt;Consistent Hashing&lt;/h4&gt;
&lt;p&gt;Dynamo's partitioning scheme relies on a variant of consistent hashing for load balancing. It applies a MD5 hash on the key to generate a 128-bit identifier, which is used to determine the storage nodes. The hash output range forms a ring, and each node in the system is assigned a position on the ring. Thus, each node is responsible for the region in the ring between it and its predecessor node. To know the detail about consistent hashing, please refer to this blog &lt;a href="http://www.tomkleinpeter.com/2008/03/17/programmers-toolbox-part-3-consistent-hashing"&gt;post&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Virtual Node&lt;/h4&gt;
&lt;p&gt;Dynamo uses the concept of "virtual nodes", and each physical node can be responsible for more than one virtual node on the ring. Using virtual nodes makes the key distribution load balancing more fine-grained and more uniform:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The load handled by an unavailable node is evenly dispersed across the remaining available nodes.&lt;/li&gt;
&lt;li&gt;A new node accepts a roughly equivalent of load from each of the other available nodes.&lt;/li&gt;
&lt;li&gt;The number of virtual nodes that a physical node is responsible can be decided by its capacity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Replication&lt;/h4&gt;
&lt;p&gt;To achieve high availability, each data is replicated at N storage hosts. Each key is assigned to a coordinator node. The coordinator stores each key locally and replicates at the N-1 clockwise successor nodes in the ring. The list of nodes responsible for storing a particular key is called the &lt;strong&gt;preference list&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Object Versioning&lt;/h3&gt;
&lt;p&gt;Dynamo provides &lt;a href="http://en.wikipedia.org/wiki/Eventual_consistency"&gt;eventual consistency&lt;/a&gt;, which allows for updates on replicas &lt;strong&gt;asynchronously&lt;/strong&gt;. Dynamo treats the result of every modification as a new and immutable version of data. It uses &lt;strong&gt;vector clocks&lt;/strong&gt; to capture causality between different versions of the same object. One verctor clock is associated with every version of every object. Upon processing a read request, Dynamo detects conflicts and employs application-assisted conflict resolution if necessary. &lt;/p&gt;
&lt;h3&gt;Sloppy Quorum&lt;/h3&gt;
&lt;p&gt;Dynamo uses a "sloppy quoram" instead of strict quorum. All read and write operations are performed on the first N healthy nodes in the preference list, skipping those that are down or inaccessible. To maintain consistency among its replicas, Dynamo uses a consistency protocol similar to quorum-based voting. This protocol has two key configurable values: R and W. R is the minimum number of nodes that must participate in a successful read operation. W is the minimum number of nodes that must participate in a successful write operation. Setting R and W such that R + W &amp;gt; N yields a quorum-like system. &lt;/p&gt;
&lt;h3&gt;Merkle Tree&lt;/h3&gt;
&lt;p&gt;A &lt;a href="http://en.wikipedia.org/wiki/Merkle_tree"&gt;Merke tree&lt;/a&gt; is a hash tree where leaves are hashes of the values of individual keys. Parent nodes higher in the tree are hashes of their respective children. If the hash values of the root of two trees are equal, then the values of the leaf nodes in the tree are equal and the nodes require no synchronization. If not, it implies that the values of some replicas are different. &lt;/p&gt;
&lt;p&gt;Dynamo uses Merkle Trees for anti-entropy to keey the replicas synchronized. Each virtual node maintains a seperate Merkle tree for each key range it hosts. This allows nodes to compare whether the keys within a key range are up-to-date.&lt;/p&gt;
&lt;h3&gt;Gossip-based Membership and Failure Detection&lt;/h3&gt;
&lt;p&gt;Decentralized failure detection protocols use a simple gossip-style protocol that enable each node in the system to learn about the arrival or departure of other nodes.&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://muratbuffalo.blogspot.com/2010/11/dynamo-amazons-highly-available-key.html"&gt;Dynamo: Amazon's highly available key-value store&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jsensarma.com/blog/?p=55"&gt;Dynamo: A flawed architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://docs.basho.com/riak/1.3.2/references/dynamo/"&gt;Riak docs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary><category term="key-value store"></category></entry><entry><title>Distributed Key-Value Store</title><link href="http://www.raydevblog.us/posts/2014/distributed-key-value-store.html" rel="alternate"></link><updated>2014-04-10T22:07:00-07:00</updated><author><name>Ray Chen</name></author><id>tag:www.raydevblog.us,2014-04-10:posts/2014/distributed-key-value-store.html</id><summary type="html">&lt;p&gt;Special thanks go to Berkeley CS162 course providing a nice document and starter code for 
a distributed key-value store system design.&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;This distributed key-value storage system has multiple clients communicating 
with a single master server in a given messaging format. The master server contains a set-associative 
cache, and it uses the cache to serve GET requests without going to the key-value slave servers it
coordinates. The slave servers are contacted for a GET request only upon a cache miss on the master. 
The master will forward PUT and DEL client requests to multiple slave servers and follow the two-phase 
commit protocol (2PC) for atomic PUT and DEL operations across multiple slave servers.&lt;/p&gt;
&lt;p&gt;The figure below shows an example distributed key-value storage system.
Three clients send a master server simultaneous requests, and the master server coordinates with three
slave servers.
&lt;img alt="Alt text" src="http://www.raydevblog.us/images/distributed_kvstore_master.jpeg" /&gt;&lt;/p&gt;
&lt;h2&gt;Registration&lt;/h2&gt;
&lt;p&gt;Slave servers will send to the master a registration message with a 64-bit globally unique ID 
when they start.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The master listens for registration requests from slaves on port 9090.&lt;/li&gt;
&lt;li&gt;When a slave starts it should listen in a random free port for 2PC requests, and register that port
number with the master so that the master can send requests to it.&lt;/li&gt;
&lt;li&gt;Assuming no errors regarding registration, the master sends a response and the slave accepts the response.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Consistent Hashing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Each key will be stored using 2PC in &lt;strong&gt;two&lt;/strong&gt; slave servers; the first of them will be selected using
consistent hashing, while the second will be placed in the successor of the first one.&lt;/li&gt;
&lt;li&gt;The master will hash the key to 64-bit address space, because each slave server has a unique 64-bit ID.&lt;/li&gt;
&lt;li&gt;Each slave will store the first copies of key with hash values greater than the ID of its immediate
predecessor up to its own ID, and also the keys whose first copies are stored in its predecessor. &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Two-phase Commit&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The master will select replica locations using consistent hashing.&lt;/li&gt;
&lt;li&gt;A slave will send vote-abort to the master if the key does not exist for DEL, or invalid key/value;&lt;/li&gt;
&lt;li&gt;When sending phase-1 requests, the master must contact slaves, even if the first slave sends an abort. The master
does this by sequentially making the requests or concurrently by forking off threads.&lt;/li&gt;
&lt;li&gt;Only a single 2PC operation can be executed at a time. The master does not support concurrent update
operations across different keys, but GET operations of different keys must be concurrent unless restricted
by an ongoing update operation on the same set.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Failures, Timeouts, and Recovery&lt;/h2&gt;
&lt;p&gt;For this particular design, assume that the master will never go down. However, slave servers must log
necessary information to survive from failures.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When the slave comes back with the same ID, it will be rebuilt using the log, and know if the last 
request it received was a phase-1 or phase-2 request.&lt;/li&gt;
&lt;li&gt;If a slave crashes during phase-1, if master does not get a vote within a single timeout period, it should
assume the slave voted abort.&lt;/li&gt;
&lt;li&gt;If a slave crashes during phase-2, the master must retry until it receives a response to its decision. 
Note that when the slave restarts, it may bind to a new port and re-register. The master must retry with
the latest port the slave has registered with. &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Components&lt;/h2&gt;
&lt;p&gt;Master Server = TPCMaster + SocketServer attached (port 8080) with KVClientHandler
TPCMaster = Master Cache + SocketServer attached (port 9090) with TPCRegistrationHandler
Slave Server = TPCLog + KVServer + SocketServer (free port) attached with TPCMasterHandler&lt;/p&gt;
&lt;h2&gt;Java Source Code Breakdown&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;TPCMasterHandler.java: a network handler to handler 2PC operation requests from the master server.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The latest Java source code of my implementation is available in my &lt;a href="https://github.com/garudareiga/computer_system_design/tree/master/distributed_kvstore/src/edu/berkeley/cs162"&gt;github repository&lt;/a&gt;&lt;/p&gt;</summary><category term="key-value store"></category><category term="concurrency"></category><category term="distributed"></category><category term="two-phase commit"></category><category term="consistent hashing"></category></entry><entry><title>A Single Server Key-Value Store</title><link href="http://www.raydevblog.us/posts/2014/a-single-server-key-value-store.html" rel="alternate"></link><updated>2014-03-31T22:11:00-07:00</updated><author><name>Ray Chen</name></author><id>tag:www.raydevblog.us,2014-03-31:posts/2014/a-single-server-key-value-store.html</id><summary type="html">&lt;p&gt;On the basis of Bekerley CS163 project 3, I will implement a single-node key-value storage system using Java.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Multiple clients will be communicating with a single-node key-value server by sending and receiving
formatted messages through sockets. The sever uses a thread pool to support concurrent operations
accross multiple sets and a set-associative cache, which is backed by a disk storage.&lt;/p&gt;
&lt;p&gt;The figure below shows a single-node key-value server with three clients making simultaneous requests:
&lt;img alt="Alt text" src="http://www.raydevblog.us/images/kvstore.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;Requirements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The key-value server will support 3 interfaces:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Value GET (Key k)&lt;/em&gt;: Retrieves the key-value pair corresponding to the provided key.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;PUT (Key k, Value v)&lt;/em&gt;: Inserts the key-value pair into the store.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;DEL (Key k)&lt;/em&gt;: Removes the key-value pair corresponding to the provided key from the store.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The key-value server has a set-associative cache with the second-change eviction policy within each set.
  Each set in the cache will have a fixed number of entries, and evict entries using the second-chance algorithm.
  The cache follows a write-through caching policy. If a key exists in the cache for a &lt;em&gt;GET&lt;/em&gt; request, do not access
  the store.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All requests (get/put/del) are atomic in that they must modify the state of both the cache and the store together.&lt;br /&gt;
  Requests must be parallel across different sets and serial with the same set. The threadpool in the server shall
  maintain a queue of tasks, assign free threads to tasks and execute them asynchronously.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The server will create a serversocket that listens on a port for connections, and service requests from the client.
  A socket shall be passed to the client handler for each request that comes in.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Source Code&lt;/h2&gt;
&lt;p&gt;The latest source code of my implementation is available in my &lt;a href="https://github.com/garudareiga/computer_system_design/tree/master/kvstore/src/edu/berkeley/cs162"&gt;github repository&lt;/a&gt;&lt;/p&gt;</summary><category term="key-value store"></category><category term="concurrency"></category></entry></feed>